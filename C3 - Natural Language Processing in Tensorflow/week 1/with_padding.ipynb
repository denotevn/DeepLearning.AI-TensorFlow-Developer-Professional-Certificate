{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Text to Sequences**\n",
    "+ In the previous lab, you saw how to generate a word_index dictionary to generate tokens for each word in your corpus.\n",
    "+ You can use then use the result to convert each of the input sentences into a sequence of tokens. That is done using the texts_to_sequences() method as shown below.\n",
    "### **2. Padding**\n",
    "+ You will usually need to pad the sequences into a uniform length because that is what your model expects. You can use the pad_sequences for that.\n",
    "+ By default, it will pad according to the length of the longest sequence. You can override this with the **maxlen** argument to define a specific length. Feel free to play with the other arguments shown in class and compare the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [ \n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog',\n",
    "    'Do you love me',\n",
    "    'Do you think my dog is amayzing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'love': 2, 'my': 3, 'dog': 4, 'you': 5, 'i': 6, 'do': 7, 'cat': 8, 'me': 9, 'think': 10, 'is': 11, 'amayzing': 12}\n",
      "[[6, 2, 3, 4], [6, 2, 3, 8], [5, 2, 3, 4], [7, 5, 2, 9], [7, 5, 10, 3, 4, 11, 12]]\n",
      "[[ 6  2  3  4  0  0  0  0  0  0]\n",
      " [ 6  2  3  8  0  0  0  0  0  0]\n",
      " [ 5  2  3  4  0  0  0  0  0  0]\n",
      " [ 7  5  2  9  0  0  0  0  0  0]\n",
      " [ 7  5 10  3  4 11 12  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "padded = pad_sequences(sequences, padding='post',\n",
    "                    truncating='post', maxlen=10) # maxlen-chieu dai cau lon nhat\n",
    "\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Out-of-vocabulary tokens**\n",
    "+ Notice that you defined an oov_token when the Tokenizer was initialized earlier.\n",
    "+ This will be used when you have input words that are not found in the word_index dictionary. For example, you may decide to collect more text after your initial training and decide to not re-generate the word_index\n",
    "+ You will see this in action in the cell below. Notice that the token 1 is inserted for words that are not found in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV': 1, 'love': 2, 'my': 3, 'dog': 4, 'you': 5, 'i': 6, 'do': 7, 'cat': 8, 'me': 9, 'think': 10, 'is': 11, 'amayzing': 12}\n",
      "\n",
      "Test Sequence =  [[6, 1, 2, 3, 4], [3, 4, 1, 3, 1]]\n",
      "\n",
      "Padded Test Sequence: \n",
      "[[0 0 0 0 0 6 1 2 3 4]\n",
      " [0 0 0 0 0 3 4 1 3 1]]\n"
     ]
    }
   ],
   "source": [
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]\n",
    "\n",
    "# Generate the sequences\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "# Print the word index dictionary\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "\n",
    "# Print the sequences with OOV\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "# Print the padded result\n",
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tuanenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89cd89414f6adff44ce5da0d1957f762e9a6261316ac0024f239801a13d44ed9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
